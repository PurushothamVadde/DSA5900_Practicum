{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (from selenium) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of url's where each url in the list is yelp page url for combination of zipcode and category\n",
    "def Extracting_Yelp_Main_URl_By_Zipcodes(category, zipcodes):\n",
    "    zipcode_URL = []\n",
    "    for zipcode in zipcodes:\n",
    "        zipcode_URL.append('https://www.yelp.com/search?find_desc={0}&find_loc={1}&start=0'.format(category,zipcode))\n",
    "    return zipcode_URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the URL of pages which has the business links\n",
    "def Extracting_URL(main_url, links):\n",
    "    \n",
    "    # initiating the webdriver. Parameter includes the path of the webdriver.\n",
    "    driver = webdriver.Chrome('../chromedriver/chromedriver.exe')\n",
    "    driver.get(main_url)\n",
    "    # this is just to ensure that the page is loaded\n",
    "#     time.sleep(3)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    \n",
    "    # Now, we apply beautiful soup to html variable\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # finding all tags in the soup\n",
    "    tags = soup('a', href=True)\n",
    "    # checking each tag and appending to the links list if it is valid\n",
    "    for tag in tags:\n",
    "        if \"start\" in tag[\"href\"]:\n",
    "            if ('www.yelp.com/search?find_desc=' in tag['href'] ) and (tag['href'] not in links) and ('login' not in tag['href']) and ('signup'not in tag['href'])and ('biz' not in tag['href']):\n",
    "                links.append(tag['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the Business URl from the page\n",
    "\n",
    "def Extracting_Business_URL(main_url, base_url, Business_links):\n",
    "\n",
    "    # initiating the webdriver. Parameter includes the path of the webdriver.\n",
    "    driver = webdriver.Chrome('../chromedriver/chromedriver.exe')\n",
    "    driver.get(main_url)\n",
    "    # this is just to ensure that the page is loaded\n",
    "#     time.sleep(3)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup.find_all(\"a\", class_=\"css-166la90\")\n",
    "    \n",
    "    # checking each tag and if it is a business tag appending to the Business list if it is valid\n",
    "    for tag in tags:\n",
    "        if (\"osq\" in tag[\"href\"]) :\n",
    "            Business_links.append(base_url + tag['href'])\n",
    "\n",
    "    return Business_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main function accepts the below params\n",
    "#  base_url : yelp url \n",
    "#  main_url : url obtained from category and zipcode combination\n",
    "#  counter  : counter to know count of present url\n",
    "#  city     : selected city\n",
    "#  category : selected category\n",
    "\n",
    "\n",
    "def main(base_url,main_url,current_zipcode, city, category):\n",
    "    \n",
    "    # list to store the url of pages with business list\n",
    "    links = []\n",
    "    \n",
    "    # list to store the list of business url\n",
    "    Business_links = []\n",
    "    \n",
    "    # Extracting all page urls with business list\n",
    "    for i in range(1, 6):       \n",
    "        #calling the function to extract the url pages with business url\n",
    "        links = Extracting_URL(main_url, links)       \n",
    "        # setting the last url of the list from the result of above function as main_url\n",
    "        main_url = links[-1]\n",
    "                 \n",
    "    for link in links:\n",
    "        #calling the function to extract the business url from the function\n",
    "        Business_links=Extracting_Business_URL(link, base_url, Business_links)\n",
    "        \n",
    "    # Saving all the Business URL to csv file\n",
    "    df = pd.DataFrame({'Business_links': Business_links})\n",
    "    df.to_csv('../Data_Business_URL_City_and_Category_Wise/{0}/{1}/Business_links_{2}.csv'.format(city, category, current_zipcode))\n",
    "    \n",
    "    print(len(Business_links))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting time: time.struct_time(tm_year=2021, tm_mon=4, tm_mday=3, tm_hour=22, tm_min=18, tm_sec=22, tm_wday=5, tm_yday=93, tm_isdst=1)\n",
      "0 https://www.yelp.com/search?find_desc=HVAC&find_loc=73101&start=0\n",
      "174\n"
     ]
    }
   ],
   "source": [
    "if __name__== '__main__':\n",
    "    \n",
    "    # Loading the Zipcodes from the json file\n",
    "    # Zipcode_data is a dictonary object with 'key'=city name and 'values' = zipcodes for city \n",
    "    with open('../Data_HTML_Tags/zipcodes.json') as f:\n",
    "        zipcode_data = json.load(f)\n",
    "     \n",
    "    #####################################Input Data#############################################   \n",
    "    # Reading the input data city and category from json file\n",
    "    with open('../Data_HTML_Tags/Input.json') as f:\n",
    "        Input_data = json.load(f)\n",
    "    \n",
    "    city = Input_data['city']\n",
    "    category = Input_data['category']\n",
    "    \n",
    "    zipcodes = zipcode_data[city]\n",
    "    \n",
    "    print(\"starting time:\", time.localtime(time.time()))\n",
    "    zipcode_URL = Extracting_Yelp_Main_URl_By_Zipcodes(category, zipcodes)\n",
    "#     print(len(zipcode_URL))\n",
    "    \n",
    "    for i in range(0,len(zipcode_URL),1):\n",
    "        try:\n",
    "            # base URl is the main yelp URL\n",
    "            base_url = 'https://www.yelp.com'\n",
    "            \n",
    "            # main_url is the intial search page url with results based on category and zipcode\n",
    "            main_url = zipcode_URL[i]\n",
    "            current_Zipcode = zipcodes[i]\n",
    "            \n",
    "            # parsing the url to make sure the url is correct\n",
    "            parse = urlparse(main_url)\n",
    "            main_url= parse.geturl()\n",
    "            print(i,main_url)\n",
    "                \n",
    "            # calling the main function\n",
    "            main(base_url, main_url, current_Zipcode, city, category) \n",
    "        \n",
    "#         break\n",
    "        except:\n",
    "            print('failed URL')\n",
    "            print(i,main_url)\n",
    "            pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of Data frame with Duplicate URL : 174\n",
      "The length of DataFrame after droping the Duplicates : 131\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def Remove_Duplicates(DataFrame, path, city, category):\n",
    "    for fname in glob.glob(path):\n",
    "        df = pd.read_csv(fname)\n",
    "        DataFrame = DataFrame.append(df, ignore_index=True)\n",
    "\n",
    "    # Dropping the Duplicate values:\n",
    "    print(\"The length of Data frame with Duplicate URL :\",len(DataFrame))\n",
    "    DataFrame = DataFrame.drop_duplicates(subset=\"Business_links\")\n",
    "    print('The length of DataFrame after droping the Duplicates :', len(DataFrame))\n",
    "\n",
    "    DataFrame.to_csv('../Data_Business_URL_Links/Business_links_{0}_{1}.csv'.format(category, city))\n",
    "\n",
    "    return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #####################################Input Data#############################################   \n",
    "    # Reading the input data city and category from json file\n",
    "    with open('../Data_HTML_Tags/Input.json') as f:\n",
    "        Input_data = json.load(f)\n",
    "    \n",
    "    city = Input_data['city']\n",
    "    category = Input_data['category']\n",
    "    \n",
    "    \n",
    "    DataFrame = pd.DataFrame()\n",
    "    path = '../Data_Business_URL_City_and_Category_Wise/{0}/{1}/Business_links_*.csv'.format(city,category)\n",
    "\n",
    "    Remove_Duplicates(DataFrame, path, city, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
