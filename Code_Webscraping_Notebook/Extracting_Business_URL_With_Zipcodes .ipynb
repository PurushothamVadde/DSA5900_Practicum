{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (from selenium) (1.25.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of url's where each url in the list is yelp page url for combination of zipcode and category\n",
    "def Extracting_Yelp_Main_URl_By_Zipcodes(category, zipcodes):\n",
    "    zipcode_URL = []\n",
    "    for zipcode in zipcodes:\n",
    "        zipcode_URL.append('https://www.yelp.com/search?find_desc={0}&find_loc={1}&start=0'.format(category,zipcode))\n",
    "    return zipcode_URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the URL of pages which has the business links\n",
    "def Extracting_URL(main_url, links):\n",
    "    \n",
    "    # initiating the webdriver. Parameter includes the path of the webdriver.\n",
    "    driver = webdriver.Chrome('../chromedriver/chromedriver.exe')\n",
    "    driver.get(main_url)\n",
    "    # this is just to ensure that the page is loaded\n",
    "#     time.sleep(3)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    \n",
    "    # Now, we apply beautiful soup to html variable\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # finding all tags in the soup\n",
    "    tags = soup('a', href=True)\n",
    "    # checking each tag and appending to the links list if it is valid\n",
    "    for tag in tags:\n",
    "        if \"start\" in tag[\"href\"]:\n",
    "            if ('www.yelp.com/search?find_desc=' in tag['href'] ) and (tag['href'] not in links) and ('login' not in tag['href']) and ('signup'not in tag['href'])and ('biz' not in tag['href']):\n",
    "                links.append(tag['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the Business URl from the page\n",
    "\n",
    "def Extracting_Business_URL(main_url, base_url, Business_links):\n",
    "\n",
    "    # initiating the webdriver. Parameter includes the path of the webdriver.\n",
    "    driver = webdriver.Chrome('../chromedriver/chromedriver.exe')\n",
    "    driver.get(main_url)\n",
    "    # this is just to ensure that the page is loaded\n",
    "#     time.sleep(3)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup.find_all(\"a\", class_=\"css-166la90\")\n",
    "    \n",
    "    # checking each tag and if it is a business tag appending to the Business list if it is valid\n",
    "    for tag in tags:\n",
    "        if (\"osq\" in tag[\"href\"]) :\n",
    "            Business_links.append(base_url + tag['href'])\n",
    "            \n",
    "\n",
    "    return Business_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main function accepts the below params\n",
    "#  base_url : yelp url \n",
    "#  main_url : url obtained from category and zipcode combination\n",
    "#  counter  : counter to know count of present url\n",
    "#  city     : selected city\n",
    "#  category : selected category\n",
    "\n",
    "def main(base_url,main_url,current_zipcode, city, category):\n",
    "    \n",
    "    # list to store the url of pages with business list\n",
    "    links = []\n",
    "    \n",
    "    # list to store the list of business url\n",
    "    Business_links = []\n",
    "    \n",
    "    # Extracting all page urls with business list\n",
    "    for i in range(1, 6):       \n",
    "        #calling the function to extract the url pages with business url\n",
    "        links = Extracting_URL(main_url, links)\n",
    "        # setting the last url of the list from the result of above function as main_url\n",
    "        if len(links)>0:\n",
    "            main_url = links[-1]\n",
    "            \n",
    "    if len(links)>0:             \n",
    "        for link in links:\n",
    "            #calling the function to extract the business url from the function\n",
    "            Business_links=Extracting_Business_URL(link, base_url, Business_links)\n",
    "    else:\n",
    "        Business_links=Extracting_Business_URL(main_url, base_url, Business_links)\n",
    "        \n",
    "    # Saving all the Business URL to csv file\n",
    "    df = pd.DataFrame({'Business_links': Business_links})\n",
    "    df.to_csv('../Data_Business_URL_City_and_Category_Wise/{0}/{1}/Business_links_{2}.csv'.format(city, category, current_zipcode))\n",
    "    \n",
    "#     print(len(Business_links))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting time: time.struct_time(tm_year=2021, tm_mon=4, tm_mday=19, tm_hour=12, tm_min=26, tm_sec=23, tm_wday=0, tm_yday=109, tm_isdst=1)\n",
      "0 https://www.yelp.com/search?find_desc=Tires&find_loc=10025&start=0\n"
     ]
    }
   ],
   "source": [
    "if __name__== '__main__':\n",
    "    \n",
    "    # Loading the Zipcodes from the json file\n",
    "    # Zipcode_data is a dictonary object with 'key'=city name and 'values' = zipcodes for city \n",
    "    with open('../Data_HTML_Tags/zipcodes.json') as f:\n",
    "        zipcode_data = json.load(f)\n",
    "     \n",
    "    #####################################Input Data#############################################   \n",
    "    # Reading the input data city and category from json file\n",
    "    with open('../Data_HTML_Tags/Input.json') as f:\n",
    "        Input_data = json.load(f)\n",
    "    \n",
    "    city = Input_data['city']\n",
    "    category = Input_data['category']\n",
    "    \n",
    "    zipcodes = zipcode_data[city]\n",
    "    \n",
    "    print(\"starting time:\", time.localtime(time.time()))\n",
    "    zipcode_URL = Extracting_Yelp_Main_URl_By_Zipcodes(category, zipcodes)\n",
    "#     print(len(zipcode_URL))\n",
    "    \n",
    "    for i in range(0,len(zipcode_URL),1):\n",
    "#         try:\n",
    "            # base URl is the main yelp URL\n",
    "        base_url = 'https://www.yelp.com'\n",
    "            \n",
    "            # main_url is the intial search page url with results based on category and zipcode\n",
    "        main_url = 'https://www.yelp.com/search?find_desc=Tires&find_loc=10025&start=0'\n",
    "#             main_url = zipcode_URL[i]\n",
    "        current_Zipcode = zipcodes[i]\n",
    "            \n",
    "            # parsing the url to make sure the url is correct\n",
    "        parse = urlparse(main_url)\n",
    "        main_url= parse.geturl()\n",
    "        print(i,main_url)             \n",
    "            # calling the main function\n",
    "        main(base_url, main_url, current_Zipcode, city, category)   \n",
    "            \n",
    "        break\n",
    "            \n",
    "#         except:\n",
    "#             print('failed URL')\n",
    "#             print(i,main_url)\n",
    "#             pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of Data frame with Duplicate URL : 156525\n",
      "The length of DataFrame after droping the Duplicates : 156525\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def Remove_Duplicates(DataFrame, path, city, category):\n",
    "    for fname in glob.glob(path):\n",
    "        df = pd.read_csv(fname)\n",
    "        DataFrame = DataFrame.append(df, ignore_index=True)\n",
    "\n",
    "    # Dropping the Duplicate values:\n",
    "    print(\"The length of Data frame with Duplicate URL :\",len(DataFrame))\n",
    "    DataFrame = DataFrame.drop_duplicates(subset=\"Business_links\")\n",
    "    print('The length of DataFrame after droping the Duplicates :', len(DataFrame))\n",
    "\n",
    "    DataFrame.to_csv('../Data_Business_URL_Links/Business_links_{0}_{1}.csv'.format(category, city))\n",
    "\n",
    "    return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #####################################Input Data#############################################   \n",
    "    # Reading the input data city and category from json file\n",
    "    with open('../Data_HTML_Tags/Input.json') as f:\n",
    "        Input_data = json.load(f)\n",
    "    \n",
    "    city = Input_data['city']\n",
    "    category = Input_data['category']\n",
    "    \n",
    "    \n",
    "    DataFrame = pd.DataFrame()\n",
    "    path = '../Data_Business_URL_City_and_Category_Wise/{0}/{1}/Business_links_*.csv'.format(city,category)\n",
    "\n",
    "    Remove_Duplicates(DataFrame, path, city, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = webdriver.Chrome('../chromedriver/chromedriver.exe')\n",
    "driver.get('https://www.yelp.com/')\n",
    "html = driver.page_source\n",
    "driver.quit()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([79.25279283, 85.36994174, 85.36214242, 76.68056279, 89.34306569, 88.53092784, 85.55357239, 74.59921625])\n"
     ]
    }
   ],
   "source": [
    "print(Category_wise.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'dict_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-11d9491be473>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# sns.set_theme(style=\"whitegrid\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# tips = sns.load_dataset(\"tips\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCategory_wise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCategory_wise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'dict_values'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict_keys'>\n"
     ]
    }
   ],
   "source": [
    "key = Category_wise.keys()\n",
    "print(type(key))\n",
    "value =Category_wise.values()\n",
    "# for i in range(len(Category_wise)):\n",
    "#     print(key[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
